{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands on with renforcement learning\n",
    "# MUlti armed bandit is a simple environmnet where we understand what is going on\n",
    "# Agent side of the loop in the renforemnt learning of the network\n",
    "# Create a Bandit with 4 arms w/ python and numpy and how it relates to the CArtPole environment\n",
    "# Creating an agnet to solve the MAB problem using python and tensorflow\n",
    "# Training the agend, and understandint it's learning\n",
    "# Due to the simplicity of the environment it's very easy to understand what is going on\n",
    "# Drilling what policy search in the environment to solve this problem\n",
    "# let's train the agend so it goes through many loops of the MAB Environment\n",
    "# THis agend will be trained to performe the right action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5b63b002ebfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a bandit with 4 arms using python and np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Multi armed badnit: comes form 1 arm bandit to describe a slot machine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# YOu pull and either win or lose (+ 1 or -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Create a bandit with 4 arms using python and np\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Multi armed badnit: comes form 1 arm bandit to describe a slot machine\n",
    "# YOu pull and either win or lose (+ 1 or -1) \n",
    "#Imagine if you have multiple slot mahcines, each with a probabiltiy\n",
    "# You can either use 1 to see what it pays out\n",
    "# OR you can explore several machines\n",
    "# THis is called the exploration exploitation trade off\n",
    "\n",
    "class MultiARmedBandit:\n",
    "    def __init__(self):\n",
    "        self.bandit = [.2,.0,.1,-4.0] # WE will generate a random number every time with each probability \n",
    "        # 4th arm (Or forth value of teh array) has the highest ammount to pay off\n",
    "        self.num_actions = 4\n",
    "    def pull(self,arm):\n",
    "            return i if np.random.randn(1) > self.bandit[arm] else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression cannot contain assignment, perhaps you meant \"==\"? (<ipython-input-6-03d5db1d9251>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-03d5db1d9251>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    self.action_in = tf.placeholder(tf.int32,[1]. name='action_in')\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expression cannot contain assignment, perhaps you meant \"==\"?\n"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self,actions=4):\n",
    "        self.num_actions = actions\n",
    "        self.reward_in = tf.placeholder(tf.float32,[1],name='reward_in')\n",
    "        self.action_in = tf.placeholder(tf.int32,[1]. name='action_in')\n",
    "        \n",
    "        self.W = tf.get_variable('W', [self.num_actions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infinate -Arm Thompson Sampling\n",
    "link: https://learning.oreilly.com/videos/hands-on-artificial-intelligence/9781788391863/9781788391863-video5_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfinateThomsponSampler:\n",
    "    @staticmethod\n",
    "    def sample_arm(means,std_errors):\n",
    "        \"\"\" takes arm statistics and returns a sample index. \n",
    "            Std_error is deviation in the mean, called teh standard error.\n",
    "            \"\"\"\n",
    "        samples=means.copy()\n",
    "        for index in range(len(means)):\n",
    "            if std_errs[index] > 0:\n",
    "                samples[index] = np.random.normal(means[index],std_errs[index])\n",
    "        #Define the mean and std_dev of value for a new arm\n",
    "        new_mean, new_std= mean_std(biased(means)) #Prevent deterministic non-sampling\n",
    "        samples.append(np.random.normal(new_mean,new_std))\n",
    "        max_ind=max(range(len(Samples)),key=lambda index: samples[index])\n",
    "        return max_ind\n",
    "    @staticmethod\n",
    "    def sample_from_scores(scores_matric):\n",
    "        \"\"\"scores_matrix is a list of lists of scores\"\"\"\n",
    "        means=[]\n",
    "        std_errs=[]\n",
    "        for scores_list in scores_matrix:\n",
    "            mean,ste= mean_Ste(baised(scores_list))\n",
    "            means.append(mean)\n",
    "            std_errs.append(ste)\n",
    "        return INfinateThompsonSampler.sample_arms(mean,std_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Test infinate arm-sampling\n",
    "max_trials = 1000\n",
    "means = [np.random.uniform() for arm in range(max_trials)]\n",
    "print(\"sample means: \", mean[:10])\n",
    "plt.plot(mean,[1 for i in range(max_trials)],\".\")\n",
    "plt.show()\n",
    "\n",
    "all_cumulative_rewards=[]\n",
    "sampler = InfinateThompsonSampler\n",
    "print(sampler)\n",
    "cumulative_rewards = []\n",
    "cumulative_reward=0.0\n",
    "scores = []\n",
    "for trials in range(max_trials):\n",
    "    index = sampler.sample_from_scores(scores)\n",
    "    if index >= len(scores): # if new arm\n",
    "        scores.append([])\n",
    "    reward = np.random.binomial(1,mean[index])\n",
    "    cumulative_reward += reward\n",
    "    cumulative_rewards.append(cumulative_reward)\n",
    "    scores[index].append(reward)\n",
    "all_cumulative_rewards.append(cumulative_rewards)\n",
    "print(\"Sampled from: \",len(totals))\n",
    "plt.plot(range(max_trials),range(max)trials)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
